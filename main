(defun print_matrix (matrix)
  (format t "~%(~a" (car matrix))
  (mapcar (lambda (row) (format t "~% ~a" row)) (cdr matrix))
  (format t ")"))

(defun random_matrix (rows columns)
  ;(mapcar #'random (make-list 10 :initial-element 101))
  (mapcar (lambda (row) (mapcar #'random (make-list columns :initial-element 1.0))) (make-list rows :initial-element '())))

(defun transpose (matrix)
  (apply #'mapcar #'list matrix))

(defun dot (a b)
  (apply #'+ (mapcar #'* a b)))

(defun matrix_product (A B)
  (if (= (length B) (length (car A)))
    (let ((BT (transpose B)))
      (mapcar 
        (lambda (rowA)
          (mapcar 
            (lambda (colB)
              (dot rowA colB))
            BT))
        A))
    (error "invalid matrix shapes")))

(defun matrix_sum (A B)
  (mapcar (lambda (rowA rowB) (mapcar #'+ rowA rowB)) A B))

;_____________________________________________________________

(defun sum_row (A B)
  (mapcar (lambda (row) (mapcar #'+ row (car B))) A))

(defun prod (A B)
  (mapcar (lambda (Arow Brow) (mapcar #'* Arow Brow)) A B))

(defun scalar_prod (x A)
  (mapcar (lambda (Arow) (mapcar (lambda (Ai) (* Ai x)) Arow)) A))

(defun AxBt (A Bt)
  (if (= (length (car Bt)) (length (car A)))
    (mapcar 
      (lambda (rowA)
        (mapcar 
          (lambda (colB)
            (dot rowA colB))
          Bt))
      A)
    (error "invalid matrix shapes")))

(defun AtxB (At B)
  (matrix_product (transpose At) B))

;(print_matrix (transpose '((1 2) (3 5))))
;(print (dot '(1 2) '(3 5)))
;(print_matrix (matrix_product '((1 2) (3 5)) '((7) (11))))
;(print_matrix (AxBt '((1 2) (3 5)) '((7 11))))
;(print_matrix (AtxB (transpose '((1 2) (3 5))) '((7) (11))))

;_____________________________________________________________________________

(defvar *learning_rate* -0.05) ;must be negative to implement gradient descent
(defvar *batch_size* 10) ;number of samples to be fed at once during training
(defvar *classes* '((1 0 0 0 0 0 0 0 0 0)
                    (0 1 0 0 0 0 0 0 0 0)
                    (0 0 1 0 0 0 0 0 0 0)
                    (0 0 0 1 0 0 0 0 0 0)
                    (0 0 0 0 1 0 0 0 0 0)
                    (0 0 0 0 0 1 0 0 0 0)
                    (0 0 0 0 0 0 1 0 0 0)
                    (0 0 0 0 0 0 0 1 0 0)
                    (0 0 0 0 0 0 0 0 1 0)
                    (0 0 0 0 0 0 0 0 0 1))) ;one-hot-encoded classes

(defun encode (digit)
  (nth digit *classes*))

;_____________________________________________________________________________

(defclass Layer ()
  (
    (_input :accessor input)
    (_weights :accessor weights)
    (_biases :accessor biases)
    (_output :accessor output)
  )
)

(defmethod initialize-instance :after ((this Layer) &rest initargs &key n_neurons n_inputs)
  (progn
    (setf (weights this) (random_matrix n_inputs n_neurons))
    (setf (biases this) (list (make-list n_neurons :initial-element 0)))))

(defmethod forward ((this Layer) inputs)
  (setf (input this) inputs) ;pointer to input
  (setf (output this) 
    (sum_row 
      (matrix_product inputs (weights this)) 
      (biases this))))

;(defgeneric derivative (this value))
;(defgeneric backward (this error))

(defmethod derivative ((this Layer) value) (error "derivative not implemented for Layer"))

(defmethod backward ((this Layer) error)
  ;optimizes layer and returns error of previous layer
  (let ((gradient_layer nil) (gradient_weights nil)) ;declaration of variables
    ;calculate gradient_layer
    (setf gradient_layer
      (prod 
        error 
        (mapcar 
          (lambda (rowActiv) (mapcar (lambda (iActiv) (derivative this iActiv)) rowActiv)) 
          (output this))))
    ;optimize biases with gradient_layer
    (setf (biases this)
      (matrix_sum 
        (biases this) 
        (scalar_prod 
          *learning_rate*
          (list (mapcar 
            (lambda (tot_bias_error) (/ tot_bias_error (length gradient_layer)))
            (apply #'mapcar #'+ gradient_layer)
          )) ;avg of each bias error (of all samples)
        )))
    ;calculate contribution to error of previous Layer
    (let 
      ((contribution (AxBt gradient_layer (weights this)))) ;contribution to error of previous Layer
      ;optimize weights with gradient_weights
      (setf (weights this)
        (matrix_sum
          (weights this)
          (scalar_prod
            *learning_rate*
            (AtxB (input this) gradient_layer))))
      ;returning contribution to previous Layer
      contribution)))


;________


(defclass ReLU (Layer)
  ()
)

(defmethod forward :after ((this ReLU) inputs)
  (setf (output this) 
    (mapcar 
      (lambda (row) 
        (mapcar 
          (lambda (element)
            (if (> 0 element)
              0
              element)) 
          row))
      (output this))))

(defmethod derivative ((this ReLU) value)
  ;derivative of the activation function (expressed in function of the result of the activation)
  (if (> 0 value)
    0
    1))

;________

(defclass LeakyReLU (Layer)
  ()
)

(defmethod forward :after ((this LeakyReLU) inputs)
  (setf (output this) 
    (mapcar 
      (lambda (row) 
        (mapcar 
          (lambda (element)
            (if (> 0 element)
              (* element 0.1)
              element)) 
          row))
      (output this))))

(defmethod derivative ((this LeakyReLu) value)
  (if (> 0 value)
    0.1
    1))

;_______________________________________________________________
;Loss function

(defun loss (results expected_classes)
  (flet ((actual_loss (a y)
         (/ (expt (- a y) 2) 2)
         ))
    (mapcar 
      (lambda (sample_result exp_class)
        (mapcar #'actual_loss sample_result (encode exp_class)))
      results expected_classes)))

(defun loss_derivative (results expected_classes)
  (flet ((actual_derivative (a y)
         (- a y)
         ))
    (mapcar 
      (lambda (sample_result exp_class)
        (mapcar #'actual_derivative sample_result (encode exp_class)))
      results expected_classes)))


;_______________________________________________________________

(defun get-numbers-from-line (csv-file-path line-number)
  (with-open-file (stream csv-file-path :direction :input)
    (loop repeat line-number
          do (read-line stream))
    (let ((line (read-line stream)))
      (if line
          (let* ((split-line (loop for i = 0 then (1+ j)
                                   for j = (or (position #\, line :start i) (length line))
                                   collect (subseq line i j)
                                   until (= j (length line)))))
            (map 'list #'parse-integer split-line))
          (error "Line ~a not found in the CSV file." line-number)))))

;; Example usage:
;; (setq csv-file-path "C:\\Users\\Usuario\\Desktop\\EMNIST\\emnist-digits-test.csv")
;; (setq line-number 0) ; Replace this with the line number you want to extract
;; (setq numbers-list (get-numbers-from-line csv-file-path line-number))
;; (format t "Numbers List: ~a~%" numbers-list)
;_______________________________________________________________




;tests

(defvar X '((1 2 3 2.5)
                 (2 5 -1 2)
                 (-1.5 2.7 3.3 -0.8)))

(defvar weights '((0.2 0.8 -0.5 1.0)
                  (0.5 -0.91 0.26 -0.5)
                  (-0.26 -0.27 0.17 0.87)))

(defvar biases '(2 3 0.5))


(defvar layer1 (make-instance 'LeakyReLU :n_neurons 3 :n_inputs 4))
(defvar layer2 (make-instance 'LeakyReLU :n_neurons 2 :n_inputs 3))
(defvar layer3 (make-instance 'LeakyReLU :n_neurons 1 :n_inputs 2))
(defvar nn (list layer1 layer2 layer3))

(defun predict (input topology)
  (if topology
    (predict (forward (car topology) input) (cdr topology))
    input
  )
)

(forward layer1 X)
;(print_matrix (forward layer2 (output layer1)))
(print_matrix (predict X nn))


(defun optimize_nn (error rtopology)
  (if rtopology
    (optimize_nn (backward (car rtopology) error) (cdr rtopology))
  )
)
(inspect (caddr nn))
(optimize_nn (loss_derivative (predict X nn) 'here_go_the_classes) (reverse nn))


(print_matrix (predict X nn))
(inspect (caddr nn))
